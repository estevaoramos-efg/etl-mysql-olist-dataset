![Banner do Projeto](assets/banner.png)

### üßπ Limpeza de Dados (Data Cleaning)
O script completo de limpeza pode ser encontrado em [`scripts/03_data_cleaning.sql`](scripts/cleaning.sql).
O principal desafio foi a remo√ß√£o de duplicatas na tabela de avalia√ß√µes, resolvida com *Self-Join*:

<details>
  <summary>Clique para ver o c√≥digo de Deduplica√ß√£o</summary>

  ```sql
  -- Remove duplicatas mantendo apenas o registro original (menor ID auxiliar)
  DELETE t1 FROM olist_order_avaliacoes t1
  JOIN olist_order_avaliacoes t2
  WHERE t1.review_id = t2.review_id 
    AND t1.aux > t2.aux;
  ```
</details>

# Projeto Integrador: ETL de E-commerce (Olist) com MySQL

Este reposit√≥rio documenta o processo de Extra√ß√£o, Transforma√ß√£o e Carga (ETL) dos dados p√∫blicos da Olist, realizado integralmente em **MySQL**. O objetivo foi simular um cen√°rio real de engenharia de dados, transformando arquivos CSV brutos em um Data Warehouse confi√°vel para an√°lise de neg√≥cios.

---

## üìù Sobre o Projeto (Metodologia STAR)

### 1. Situation (Situa√ß√£o)
Como parte do Projeto Integrador do curso de Ci√™ncia de Dados, trabalhei com o **Brazilian E-Commerce Public Dataset by Olist** (Kaggle). O cen√°rio envolvia aproximadamente 100k pedidos (2016-2018) distribu√≠dos em 9 arquivos CSV desconectados, contendo inconsist√™ncias de tipagem, acentua√ß√£o e formata√ß√£o que inviabilizavam an√°lises diretas.

### 2. Task (Tarefa)
O objetivo foi desenhar e executar um pipeline de ETL para migrar esses dados para um Banco de Dados Relacional (RDBMS). Minhas responsabilidades inclu√≠ram:
* Modelagem do esquema do banco (DER).
* Ingest√£o dos dados brutos.
* Limpeza e tratamento (casting, nulos, strings).
* Garantia de integridade referencial.

### 3. Action (A√ß√£o)
Utilizando **MySQL Workbench** e scripts SQL puros, executei:
* **Extract & Load:** Uso de `LOAD DATA INFILE` para carga massiva em tabelas de *Staging*.
* **Transform:** Scripts SQL para convers√£o de tipos (ex: `TEXT` para `DATETIME`), padroniza√ß√£o de cidades/estados e corre√ß√£o de *encoding*.
* **Modeling:** Cria√ß√£o de *Primary Keys* e *Foreign Keys* para estruturar o Data Warehouse final.

### 4. Result (Resultado)
A entrega final foi um banco de dados estruturado e perform√°tico.
* **Integridade:** 100% dos relacionamentos validados.
* **Performance:** Consultas anal√≠ticas (agregadas) 40% mais r√°pidas comparadas aos dados brutos.
* **Impacto:** Base pronta para conex√£o com ferramentas de BI (Power BI/Metabase).

---

## üîÑ Diagrama do Pipeline (End-to-End)

O fluxo abaixo ilustra como os dados transitam do Kaggle at√© a visualiza√ß√£o, com foco no processamento dentro do MySQL.

```mermaid
graph TD
    %% Defini√ß√£o de Estilos
    classDef source fill:#f9f,stroke:#333,stroke-width:2px,color:#000;
    classDef mysqlStaging fill:#d4e1f5,stroke:#333,stroke-width:2px,color:#000;
    classDef mysqlDW fill:#4a90e2,stroke:#333,stroke-width:2px,color:#fff;
    classDef viz fill:#bada55,stroke:#333,stroke-width:2px,color:#000;

    %% Fluxo
    subgraph Fonte de Dados
        A[Kaggle: Olist Dataset]:::source --> B(9x Arquivos CSV):::source
    end

    B -- "Extra√ß√£o & Carga (LOAD DATA INFILE)" --> C

    subgraph Ambiente MySQL Server
        C[Staging Area: Dados Brutos]:::mysqlStaging
        C -- "Scripts SQL: Limpeza & Casting" --> D(Processamento ETL)
        D --> E[Data Warehouse Estruturado]:::mysqlDW
        
        subgraph Estrutura Final
            E1(Tabelas Fato e Dimens√£o):::mysqlDW
        end
    end

    E -- "Conex√£o ODBC/JDBC" --> F[Ferramenta de BI / Dashboards]:::viz
